
**1. 什么是数据仓库？**

   数据仓库（Data Warehouse）是一个面向主题的（Subject Oriented）、集成的（Integrated）、相对稳定的（Non-Volatile）、反映历史变化（Time Variant）的数据集合，数据库主要用于**事务处理**，数据仓库主要用于**数据分析**;
   
   数据库的特点：
   1. 相对复杂的表格结构，存储结构相对紧致，少冗余数据 
   2. 读和写都有优化

   数据仓库的特点：
   1. 相对简单的表结构，存储相对松散，多冗数据
   2. 一般是只读优化

**2. 你是如何使用 Spark 管理日志的？**

   1. 每次对实例的更改，都会把变更记录到 mongo 数据库，形成一个变更记录的collection；
   2. 分析数据时，将 mongo 数据库中 collection 的数据使用 Spark 倒入到 Hive 表中，然后再进行分析

**3. 你在调研图数据库的时候都调研了哪些指标，为什么最后选择了 JanusGraph + Neo4j 这两个数据库？**

   * 这里的东西需要多补充一下 ！！！

   1. OLTP：基本可以理解为 CRUD，对实时性要求高；
   2. OLAP：离线分析大量数据产生想要的结果，对实时性要求不高

   | 数据库名 | 查询语言  |  多值属性 |批量倒入 | 增量批量倒入  |  是否支持分布式 |支持导入/导出哪些数据库 |
   | :-----| ----: | :----: |:-----| ----: | :----: |:----: |
   | JanusGraph | Gremlin，有 PythonSDK | 原生支持 List，Set |支持（GraphML，GraphSon，Gryo） | 支持（GraphML，GraphSon，Gryo） | 基于 Hbase，Cassandra（API） |单元格 |
   | 阿里云GDB | Gremlin | 不支持 List | 支持CSV 格式，必须先上传到 OSS | 单元格 | 单元格 |单元格 |
   | Neo4j | Gremlin/Cypher | 支持 List | 支持CSV 格式，必须先上传到 OSS | 不支持分布式 | 单元格 |单元格 |
   | Cayley | Gizmo/GraphQL/MOL | 单元格 | n-quad | n-quad | 单元格 |单元格 |


   **为什么选用 JanusGraph ？**
   1. JanusGraph 支持强 schema
   2. JanusGraph 支持分布式部署；
   * TODO

   **为什么选用 Neo4j ？**
   * TODO

   JanusGraph：可以通过 Hadoop-Gremlin 批量倒入，同时 JanusGraph 支持通过 batch-loading 的方式，此模式下会关闭一些一致性的检验，加快写入速度；

   * 这里如果问到，着重讲解一个就可以了。

**4. 深入调研 JanusGraph，ThinkerPop，都得到了哪些成果？（这里面试官可能问到哪些问题？）**

   1. Gremlin 语法文档总结；
   2. 不同数据库对 Gremlin 支持程度的测试；
   3. HugeGraph 不能动态支持修改 schema，有些直接报错，因此放弃了 Hu个Graph；

**5. 为什么要有 Spark，他相比 Hadoop 解决了什么问题？**

   1. Spark 替代的 Hadoop 生态中的 MapReduce；
   2. MapReduce 在计算时，每次 Reduce 之后都会写入磁盘文件。如果一个任务有多个 Map/Reduce 计算，就会多次读写磁盘文件，大大降低运算速度；
   3. Spark 基于内存，计算的中间结果都优先放在内存中（内存不够再考虑磁盘），因此速度比 MapReduce 快；
   4. Spark 提供了更加丰富的 API，编程人员可以用更少的代码实现更多的事；

   * Spark 擅长迭代式计算，交互式查询，流处理，批处理（这个 Hadoop 好像也有？）；

**6. Spark 弹性体现在哪里？**

   1. 自动的进行内存和磁盘数据存储的切换；
   2. 基于 lineage 的高效容错（某处出错，不需要从头开始计算）；
   3. Task 失败会自动进行特定次数的重试；

**7. Spark 什么时候进行缓存？**

   1. 特别耗时的计算之后；
   2. 计算链条已经很长，计算之后；
   3. shuffle 之后；
   4. checkpoint（checkpoint 就是把所有的数据放到磁盘中） 之前；

**8. 什么是 RDD，什么是 Partition？**

   1. 一个 RDD 是一个只读的，分布式的，被分区的数据集，由多个 partition 构成；
   2. RDD 不可变，可以自我恢复；
   3. 一个 partition 是一些数据的集合；

**9. 计算模型为什么使用 DAG ？**

   1. DAG 是有向无环图，RDD 和 关系构成 DAG，DAG 的顶点代表 RDD，边代表对 RDD 进行的操作；
   2. DAG记录 RDD 之间直接的依赖关系，当某个 RDD 出错时，可以通过依赖关系从父 RDD 恢复出错的 RDD，体现了 DAG 良好的容错性；
   3. DAG 主要解决了 MapReduce 的两个问题：
      * 每个 MapReduce 操作都是相互独立的，HADOOP不知道接下来会有哪些 Map/Reduce；
      * 每一步的输出结果，都会持久化到硬盘或者 HDFS 上；

**10. 什么是 shuffle，哪些情况会 shuffle ？**

   1. 把分布在不同节点的数据按照一定的规则汇集到一起的过程，即把数据在不同 partition 之间移动；
   2. shuffle 分为：shuffle write，shuffle read；只有产生宽依赖的时候才会 shuffle，shuffle 会写入本地磁盘文件；
   3. 下面的操作会产生 shuffle：
      * reparation 相关的操作；repartition；
      * *Bykey 相关的操作：GroupByKey，reduceByKey，combineByKey，aggregateByKey
      * join 相关的操作：cogroup，join

   * 窄依赖：
      * 父 RDD 的一个 partition 只被子 RDD 的一个 partition 依赖；
   * 宽依赖:
      * 父 RDD 的一个 partition 被子 RDD 的多个 partition 依赖；


**11. Stage 的划分依据？**

   1. 根据 DAG 图，从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中；

**12. Spark 2.0 为什么有了 dataframe，解决了 RDD 的哪些不足？**

   1. DataFrame 有 schema，而 RDD 没有 schema；
   2. 有了 schema 之后，可以更方便，更高效的对数据进行管理，查询，分析；
   3. 有了 schema 之后才可以写 Sql 语句；
   
   比如：
      有了 schema 之后，你可以选择只需读需要的数据，加快 I/O，Hadoop 一般是列存，I/O 快

**13.  Hive 是什么，Hive 和 HDFS 的区别？**

   Hadoop 生态：
   * HDFS 文件管理系统，可以对比 Windows 的 File Explore；
   * Hive 是一个数据仓库，可以创建文件，方便管理数据，可以对比 Mysql 理解（Hive 没有事务，不支持 insert）
   * HBase（Hadoop database），是一种 key-value 的数据库
   * Map/Reduce 计算框架；

   Spark ---> Map/Reduce

**14. Mongo 中的数据库级别？**

   1. 数据量在不到千万级（400/500 万量），支持直接查询；Mongo 本身提供的功能可以满足当前的需求，查询大多在毫秒级，最多不超过 2s；
   2. log 日志在不到亿级（1亿），分析数据使用的是 Spark，怎么读 Mongo 是我们组大神写的，没有深入研究；


   